---
title: "Homework 5"
author: "DATASCI/STATS 531, Liangqi Tang"
format: 
  html:
    toc: TRUE
    toc-depth: 5
    code-fold: TRUE
    code-summary: "Code"
    code-tools: TRUE
    embed-resources: TRUE
---

\newcommand\prob{\mathbb{P}}
\newcommand\E{\mathbb{E}}
\newcommand\var{\mathrm{Var}}
\newcommand\cov{\mathrm{Cov}}

-----------

This homework gives you some experience at algebraic manipulation of POMP models by deriving the prediction, filtering and smoothing formulas in [Chapter 10](../10/index.html).

The calculations are all applications of basic definitions (such as the Markov property) and basic identities for joint, conditional and marginal probability density functions. 
The goal is to check carefully how the formulas follow from these properties, so please explain this explicitly in your solutions.
The hints for Exercises 10.2 and 10.4 in the notes may be useful.

The homework can be handwritten and scanned to pdf, however it is recommended to use Latex in Rmarkdown and submit as html. If you are relatively unfamiliar with Latex and Rmarkdown, this will take more time but it is a worthwhile exercise. As usual, you are welcome to use the Latex from the notes as a source, if you like.

Your report should contain a reference section listing sources. The grader should be able to clearly identify where the sources were used, for example using reference numbers in the text. Anything and anyone consulted while you are working on the homework counts as a source and should be credited. The most likely sources for this theoretical homework are class notes, colleagues, Piazza posts and past homework solutions. It is your responsibility to demonstrate and explain in your report the extent to which you have carried out independent work going beyond any sources listed. The homework will be graded following the grading scheme in the [syllabus](../syllabus.html).


--------

# Question 5.1. 

Derive the identity [P2].

**Answer**:

I derive the identity [P2] independently:

$$
\begin {aligned}
f_{X_{0:N}}(x_{0:N}) &= f_{X_{0:N-1}}(x_{0:N-1}) \cdot f_{X_n|X_{0:N-1}}(x_n|x_{0:N-1})\\
&= f_{X_{0:N-1}}(x_{0:N-1}) \cdot f_{X_n|X_{n-1}}(x_n | x_{n-1})\\
&= \cdots\\
&= f_{X_0}(x_0) \prod_{n=1}^N f_{X_n|X_{n-1}}(x_n|x_{n-1})
\end {aligned}
$$

For the second equation above, I used the property of Markov process (cite:[Markov Process Property (P1)](https://ionides.github.io/531w24/10/slides.pdf)):

$$
f_{X_n|X_{0:n-1}}(x_n|x_{0:n-1}) = f_{X_n|X_{n-1}} (x_n | x_{n-1})
$$

****

# Question 5.2. 

Derive the prediction formula, [P4].

**Answer**:

I derive the formula independently.

Since:

$$
f_{X_n|Y_{1:n-1}}(x_n | y^*_{1:n-1}) = \int f_{X_{n-1},X_n | Y_{1:n-1}}(x_{n-1}, x_n | y^*_{1:n-1})dx_{n-1}
$$

This is just the definition of marginal density(cite:[Wikipedia, definition of marginal density](https://en.wikipedia.org/wiki/Marginal_distribution#Marginal_probability_density_function)).

This is equivalent to prove:

$$
\int f_{X_{n-1},X_n | Y_{1:n-1}}(x_{n-1}, x_n | y^*_{1:n-1})dx_{n-1} = \int f_{X_{n-1}|Y_{1:n-1}}(x_{n-1}|y^*_{1:n-1}) f_{X_n | X_{n-1}}(x_n|x_{n-1})dx_{n-1}
$$

i.e. To prove

$$
f_{X_{n-1},X_n | Y_{1:n-1}}(x_{n-1}, x_n | y^*_{1:n-1}) =  f_{X_{n-1}|Y_{1:n-1}}(x_{n-1}|y^*_{1:n-1}) f_{X_n | X_{n-1}}(x_n|x_{n-1})
$$

Since (Markov process):

$$
f_{X_n | X_{n-1}}(x_n|x_{n-1}) = f_{X_n | X_{n-1}, Y_{1:n-1}}(x_n|x_{n-1}, y^*_{1:n-1})
$$

Thus this is just the definition of conditional probability that: (cite:[Wikipedia, conditional density](https://en.wikipedia.org/wiki/Conditional_probability_distribution))

$$
f(x_1,x_2) = f(x_1) \cdot f(x_2 | x_1)
$$

Thus the formula is derived.

****

# Question 5.3. 

Derive the filtering formulas [P5] and [P6].

**Answer**:

I derive the formula independently.

For [P5]:

Since:

$$
f_{X_n| Y_{1:n}}(x_n | y^*_{1:n}) \cdot f_{Y_n | Y_{1:n-1}}(y^*_n | y^*_{1:n-1}) = f_{X_n,Y_{n} | Y_{1:n-1}}(x_n,y_{n}| y^*_{1:n-1})
$$

And

$$
\begin {aligned}
f_{X_n|Y_{1:n-1}}(x_n | y^*_{1:n-1}) \cdot f_{Y_n|X_n}(y^*_n | x_n) &= f_{X_n|Y_{1:n-1}}(x_n | y^*_{1:n-1}) \cdot f_{Y_n|X_n, Y_{1:n-1}}(y^*_n | x_n, y^*_{1:n-1})\\
&= f_{X_n,Y_n | Y_{1:n-1}}(x_n,y_n| y^*_{1:n-1})
\end {aligned}
$$

The second equation above I use the [property of measurement model, P3](https://ionides.github.io/531w24/10/slides.pdf). Thus [P6] is derived.

Thus:

$$
f_{X_n| Y_{1:n}}(x_n | y^*_{1:n}) \cdot f_{Y_n | Y_{1:n-1}}(y^*_n | y^*_{1:n-1}) =f_{X_n|Y_{1:n-1}}(x_n | y^*_{1:n-1}) \cdot f_{Y_n|X_n}(y^*_n | x_n) 
$$

i.e.

$$
f_{X_n| Y_{1:n}}(x_n | y^*_{1:n}) =\frac{f_{X_n|Y_{1:n-1}}(x_n | y^*_{1:n-1}) \cdot f_{Y_n|X_n}(y^*_n | x_n)}{f_{Y_n | Y_{1:n-1}}(y^*_n | y^*_{1:n-1})}
$$

this is just the [Bayes formula](https://en.wikipedia.org/wiki/Bayes%27_theorem) conditional on $Y_{1:n-1}$. Thus [P5] is derived.

For [P6]:

Since:

$$
\begin {aligned}
f_{Y_n | Y_{1:n-1}}(y^*_n | y^*_{1:n-1}) &= \int f_{X_n | Y_{1:n-1}}(x_n | y^*_{1:n-1})\cdot f_{Y_n | X_n, Y_{1:n-1}}(y^*_n|x_n, y^*_{1:n-1})dx_n\\
& = \int f_{X_n | Y_{1:n-1}}(x_n | y^*_{1:n-1})\cdot f_{Y_n | X_n}(y^*_n | x_n)dx_n
\end {aligned}
$$

The second equation above I use the [property of measurement model, P3](https://ionides.github.io/531w24/10/slides.pdf). Thus [P6] is derived.

****

# Question 5.4. 

Derive the backward recursion formulas [P7] and [P8].

**Answer**:

I derive the formula independently.

For [P7]:

$$
\begin {aligned}
f_{Y_{n:N} | X_n} (y^*_{n:N} | x_n) &= f_{Y_n| X_n, Y_{n+1:N}} (y^*_n | x_n, y^*_{n+1:N}) \cdot f_{Y_{n+1:N}|X_n}(y^*_{n+1:N} | x_n) \\
&= f_{Y_n| X_n}(y^*_n|x_n)\cdot f_{Y_{n+1:N}|X_n}(y^*_{n+1:N} | x_n)
\end {aligned}
$$

Again, the second equation above I used [property of measurement model, P3](https://ionides.github.io/531w24/10/slides.pdf). Thus [P7] is derived.

For [P8]:

Since:

$$
\begin {aligned}
f_{Y_{n+1:N} | X_n} (y^*_{n+1:N} | x_n) = \int f_{Y_{n+1:N}, X_{n+1} | X_n}(y^*_{n+1:N}, x_{n+1} | x_n) dx_{n+1}
\end {aligned}
$$

This is equivalent to prove:

$$
\int f_{Y_{n+1:N}, X_{n+1} | X_n}(y^*_{n+1:N}, x_{n+1} | x_n) dx_{n+1} =  \int f_{Y_{n+1:N} | X_{n+1}} (y^*_{n+1:N} | x_{n+1}) f_{X_{n+1} | X_n} (x_{n+1} | x_n) dx_{n+1}
$$

i.e. To prove:

$$
f_{Y_{n+1:N}, X_{n+1} | X_n}(y^*_{n+1:N}, x_{n+1} | x_n) = f_{Y_{n+1:N} | X_{n+1}} (y^*_{n+1:N} | x_{n+1}) f_{X_{n+1} | X_n} (x_{n+1} | x_n)
$$

Since:

$$
f_{Y_{n+1:N} | X_{n+1}} (y^*_{n+1:N} | x_{n+1}) = f_{Y_{n+1:N} | X_{n+1}, X_n} (y^*_{n+1:N} | x_{n+1}, x_n)
$$

Thus it's just the definition of conditional density. Thus [P8] is proved.

****

# Question 5.5. 

Derive the smoothing formula [P9].

**Answer**:

I derive the formula independently with the hint of slides10 p25.

$$
\begin {aligned}
f_{X_n | Y_{1:N}} (x_n | y^*_{1:N}) \cdot f_{Y_{n:N} | Y_{1:n-1}} (y^*_{n:N} | y^*_{1:n-1}) &= f_{X_n, Y_{n:N} | Y_{1:n-1}} (x_n, y^*_{n:N} | y^*_{1:n-1})
\end {aligned}
$$

And 

$$
\begin {aligned}
f_{X_n | Y_{1:n-1}} (x_n | y^*_{1:n-1}) \cdot f_{Y_{n:N} | X_n} (y^*_{n:N} |x_n) &= 
f_{X_n | Y_{1:n-1}} (x_n | y^*_{1:n-1}) \cdot f_{Y_{n:N} | X_n, Y_{1:n-1}} (y^*_{n:N} |x_n, y^*_{1:n-1})\\
&= f_{X_n, Y_{n:N} | Y_{1:n-1}} (x_n, y^*_{n:N} | y^*_{1:n-1})
\end {aligned}
$$

In the first equation above I used the property of measurement process.

Thus [P9] is derived.

****

# Reference

- 1. [Markov Process Property (P1)](https://ionides.github.io/531w24/10/slides.pdf)
- 2. [Wikipedia, definition of marginal density](https://en.wikipedia.org/wiki/Marginal_distribution#Marginal_probability_density_function)
- 3. [Wikipedia, conditional density](https://en.wikipedia.org/wiki/Conditional_probability_distribution)
- 4. [Bayes formula](https://en.wikipedia.org/wiki/Bayes%27_theorem)
- 5. [property of measurement model, P3](https://ionides.github.io/531w24/10/slides.pdf)
- 6. [Hints and properties of slides10, p20](https://ionides.github.io/531w24/10/slides.pdf)