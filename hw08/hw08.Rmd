---
title: "Homework 8"
author: "DATASCI/STATS 531, Liangqi Tang"
format: 
  html:
    toc: TRUE
    toc-depth: 5
    code-fold: TRUE
    code-summary: "Code"
    code-tools: TRUE
    embed-resources: TRUE
---

For this Homework, I didn't collaborate with my final project group. All my solutions are based on my understanding of the slides and previous homework. Also I have referred to some resources which will be cited in the content and listed in the end of the report.

# Question 1.

**Answer**:

C. Firstly, the measurement times `time(po)` do not need to be in units of weeks. Since in Homework we set them to be weeks 1,2,... but this should still work if the time is days, months or years. It can also be 1/52, 2/52, ..., 2. What matters here is that the time interval should be the same which will match our "time step" *dt* in the model. Thus we can exclude options A and B. Besides, the time units must be match between measurement times and latent times since our [pomp theory](https://ionides.github.io/531w24/10/slides.pdf) is based on this setting, i.e. $y_t$ is a measurement of the latent process $x_t$. If $t$ between $x$'s and $y$'s do not match, it will not make any sense. Thus we can exclude options D and E.

****

**(For Question 2,3,4 I referred to the way how [previous solutions](https://github.com/LiangqiTang/Stats-531/blob/main/sol08.pdf) generate the similar error message.)**

# Question 2.

**Answer**:

E. For this question, I followed the basic debug process and input the error message in Google but I can't seem to find anything useful. So I just try to follow the errors in options and change the codes in my Homework6 until I find the similar error message: Here I miss the semicolon at the end of `I = 1` in `seir_rinit` on purpose.

```{r, message = FALSE ,error = TRUE}
library(pomp)

readr::read_csv("https://kingaa.github.io/sbied/stochsim/Measles_Consett_1948.csv") |>
  dplyr::select(week,reports=cases) -> meas

# step function: E is similar to the component I, add E in the chain
seir_step <- Csnippet("
  double dN_SE = rbinom(S,1-exp(-Beta*I/N*dt));
  double dN_EI = rbinom(E,1-exp(-mu_EI*dt));
  double dN_IR = rbinom(I,1-exp(-mu_IR*dt));
  S -= dN_SE;
  E += dN_SE - dN_EI;
  I += dN_EI - dN_IR;
  R += dN_IR;
  H += dN_IR;
")

# initial function: suppose we do not no the latent value of E at first.
seir_rinit <- Csnippet("
  S = nearbyint(eta*N);
  E = 0; 
  I = 1
  R = nearbyint((1-eta)*N);
  H = 0;
")

# dmeasure component
seir_dmeas <- Csnippet("
  lik = dnbinom_mu(reports,k,rho*H,give_log);
")

# rmeasure component
seir_rmeas <- Csnippet("
  reports = rnbinom_mu(k,rho*H);
")


# add components to pomp model: clarify E and mu_EI
meas |>
  pomp(times="week",t0=0,
    rprocess=euler(seir_step,delta.t=1/7),
    rinit=seir_rinit,
    rmeasure=seir_rmeas,
    dmeasure=seir_dmeas,
    accumvars="H",
    statenames=c("S","E","I","R","H"),
    paramnames=c("Beta","mu_EI","mu_IR","N","eta","rho","k")
  ) -> measSEIR
```

****

# Question 3

**Answer**:

B. Similar to Question 2. I try out errors in the options: I delete the `eta` in `paramnames` on purpose and got the similar error message.

```{r, message = FALSE ,error = TRUE}
library(pomp)

readr::read_csv("https://kingaa.github.io/sbied/stochsim/Measles_Consett_1948.csv") |>
  dplyr::select(week,reports=cases) -> meas

# step function: E is similar to the component I, add E in the chain
seir_step <- Csnippet("
  double dN_SE = rbinom(S,1-exp(-Beta*I/N*dt));
  double dN_EI = rbinom(E,1-exp(-mu_EI*dt));
  double dN_IR = rbinom(I,1-exp(-mu_IR*dt));
  S -= dN_SE;
  E += dN_SE - dN_EI;
  I += dN_EI - dN_IR;
  R += dN_IR;
  H += dN_IR;
")

# initial function: suppose we do not no the latent value of E at first.
seir_rinit <- Csnippet("
  S = nearbyint(eta*N);
  E = 0; 
  I = 1;
  R = nearbyint((1-eta)*N);
  H = 0;
")

# dmeasure component
seir_dmeas <- Csnippet("
  lik = dnbinom_mu(reports,k,rho*H,give_log);
")

# rmeasure component
seir_rmeas <- Csnippet("
  reports = rnbinom_mu(k,rho*H);
")


# add components to pomp model: clarify E and mu_EI
meas |>
  pomp(times="week",t0=0,
    rprocess=euler(seir_step,delta.t=1/7),
    rinit=seir_rinit,
    rmeasure=seir_rmeas,
    dmeasure=seir_dmeas,
    accumvars="H",
    statenames=c("S","E","I","R","H"),
    paramnames=c("Beta","mu_EI","mu_IR","N","rho","k")
  ) -> measSEIR
```

****

# Question 4

**Answer**

A. Similar to Question 2 and 3. I try out errors in the options: I used R syntax within a C function that has the same name as an R function in `seir_rmeas`.

```{r, message = FALSE ,error = TRUE}
library(pomp)

readr::read_csv("https://kingaa.github.io/sbied/stochsim/Measles_Consett_1948.csv") |>
  dplyr::select(week,reports=cases) -> meas

# step function: E is similar to the component I, add E in the chain
seir_step <- Csnippet("
  double dN_SE = rbinom(S,1-exp(-Beta*I/N*dt));
  double dN_EI = rbinom(E,1-exp(-mu_EI*dt));
  double dN_IR = rbinom(I,1-exp(-mu_IR*dt));
  S -= dN_SE;
  E += dN_SE - dN_EI;
  I += dN_EI - dN_IR;
  R += dN_IR;
  H += dN_IR;
")

# initial function: suppose we do not no the latent value of E at first.
seir_rinit <- Csnippet("
  S = nearbyint(eta*N);
  E = 0; 
  I = 1;
  R = nearbyint((1-eta)*N);
  H = 0;
")

# dmeasure component
seir_dmeas <- Csnippet("
  lik = dnbinom_mu(reports,k,rho*H,give_log);
")

# rmeasure component
seir_rmeas <- Csnippet("
  double mean, sd;
  double length;
  mean = rho*H;
  sd = H;
  length = rnorm(1,mean,sd);
  reports = rnbinom_mu(k,rho*H);
")


# add components to pomp model: clarify E and mu_EI
meas |>
  pomp(times="week",t0=0,
    rprocess=euler(seir_step,delta.t=1/7),
    rinit=seir_rinit,
    rmeasure=seir_rmeas,
    dmeasure=seir_dmeas,
    accumvars="H",
    statenames=c("S","E","I","R","H"),
    paramnames=c("Beta","mu_EI","mu_IR","N","eta","rho","k")
  ) -> measSEIR
```

****

# Question 5

**Answer**:

B. If $h$ is an indentity map, then $W_n = h(V_n) = V_n$ is a Markov process. If $h$ is a function such that $h(V_n)  = V_n + V_{2n}$. Then $W_n = h(V_n) = V_n + V_{2n}$, $W_{n-1} = h(V_{n-1}) = V_{n-1} + V_{2n-2}$. Then $W_n$ is not Markov process. Thus ii is correct. Since $(X_n, Y_n)$ is a POMP model, $X_n$ is the latent process. Thus $W_n = h(V_n) = X_n$ is Markov Process. Thus iv is correct.

****

# Question 6

**Answer**:

C. For one core:

$$
10 \times 10^3 = 10^4 = 15 \text{min}
$$
Then:

$$
20 \times 10^4 = 2 \times 10^5 = 20\times 15\text{min} = 300 \text{min}
$$

Equally distributed on 4 cores:

$$
300\text{min} /4 = 75 \text{min}
$$

****

# Question 7

**Answer**:

E. In this question I referred to [previous solutions](https://github.com/LiangqiTang/Stats-531/blob/main/sol08.pdf) on how to compute exact std using codes. We know from [pomp likelihood estimation](https://kingaa.github.io/sbied/pfilter/slides.pdf) that the particle filter can calculate the unbiased likelihood estimate on a natural scale but not on a log scale from the slides. Thus by the codes below:

```{r}
es <- c(-2446, -2444, -2443, -2442, -2440)
library(pomp)
logmeanexp(es, se = TRUE)
```

E is correct.

****

# Question 8

**Answer**:

A. For this question, use the codes below to read the estimates of loglikelihood and do the same operation as Question 7 using `logmeanexp` in `pomp` package:

```{r}
Q9 <- readRDS("~/Library/Mobile Documents/com~apple~CloudDocs/531/Stats531/hw08/Q9.rds")
logmeanexp(Q9)
```

Thus A is correct.

****

# Question 9

**Answer**:

D. According to [likelihood based inference for POMP](https://kingaa.github.io/sbied/pfilter/slides.pdf), if the effective sample size is too small, then the measurement information might be not enough for filtering out the noise in some time slots. This will cause the estimation on several time slots to be not convincing and thus ruin the whole particle filtering algorithm since it's sequential. Thus C is likely to happen. We should choose D.

****

# Question 10

**Answer**:

C. If the likelihood increases steadily, then it's the sign that the estimation algorithm keeps changing the parameters to get better estimation. This means that the estimation keeps improving until a convergence and it still does not make fully use of the data and "dig out" the information in it. This might imply that we misspecified the model and do not fit the data adequately. Thus C is correct.

****

# Question 11

**Answer**:

B. According to the [profile likelihood](https://kingaa.github.io/sbied/mif/slides.pdf), we can apply Wilk's Theorem only if the likelihood is profile, i.e., is the maximization of all the other parameters given one parameter. Thus if the code only involves evaluation of the likelihood but not maximization then it should be wrong.

****

# Question 12

**Answer**:

A: True. Because the profile likelihood is the maximization over every slice.

B: False. Because CI should be read from profile likelihood not slices according to Wilk's Theorem. (Only if we only have 1 parameter thus maximizing on the remaining parameters is just the likelihood itself, B can be True)

C: False.

D: True. Because the poor man's profile is just the "practical" true profile which we can compute using our parameter estimation. It should lie below the true profile.

****

# Question 13

**Answer**:

A. From the plots we can see the likelihood is climbing and all iterations and replicated searches are producing. We can also find that the $\sigma_{\nu}$ and $H_0$ do dot converge, which indicates weak identifability.

****

# Question 14

**Answer**:

E. For this question I referred to the [previous solutions](https://github.com/LiangqiTang/Stats-531/blob/main/sol08.pdf). I think B makes sense since the loglik spread too widely thus we need to increase particles and iterations to achieve better and more stable searching results. For C and D, cite from [previous solutions](https://github.com/LiangqiTang/Stats-531/blob/main/sol08.pdf):
*However, if the model fit is not great (as revealed by comparison against a benchmark) this makes the filtering harder as well as less scientifically satisfactory. If the model is fitting substantially below ARMA benchmarks, it is worth considering some extra time on model development. Identifying time points with low effective sample size can help to identify which parts of the data are problemtic for the model to explain.*.

****

**(For Question 15 and 16, I finished independently.)**

# Question 15

**Answer**:

A: False. `Nmif = 100`.

B: True. `partrans` transfer "Beta" and "alpha" to log scale.

C: False. `rw.sd` defines "gamma" as the standard deviation of a random walk.

D: True. "delta" will remain fixed since only "Beta", "alpha", "gamma" appears in `rw.sd``.

E: False.

F: True. Since the cooling fraction should be 0.1 every 50 Nmif but not every 1 Nmif.

H: True.

****

# Question 16

**Answer**:

A: True, `Nmif = 100` will be preserved and be default.

B: True, we only re-define `rw.sd` and `cooling.fraction.50`, thus all the other codes will be re-used.

C: True. The new interations will be continued at the end of the old ones.

D: True. Preserved in Question 15.

E: True. Because "gamma" is not defined in the new `rw.sd`.

F: True. Preserved in Question 15.

G: False. Since $0.2^{100/50} = 0.04 > 0.01 = 0.1^{100/50}$.

****

# Referene

- 1. [pomp theory](https://ionides.github.io/531w24/10/slides.pdf)
- 2. [previous solutions](https://github.com/LiangqiTang/Stats-531/blob/main/sol08.pdf)
- 3. [pomp likelihood estimation](https://kingaa.github.io/sbied/pfilter/slides.pdf)
- 4. [likelihood based inference for POMP](https://kingaa.github.io/sbied/pfilter/slides.pdf)
- 5. [profile likelihood](https://kingaa.github.io/sbied/mif/slides.pdf)